<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Voice Chat</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f3f4f6;
            color: #374151;
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
            padding: 1rem;
        }
        .chat-container {
            width: 100%;
            max-width: 640px;
            background-color: #ffffff;
            border-radius: 1.5rem;
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
            display: flex;
            flex-direction: column;
            height: 85vh;
            min-height: 500px;
            overflow: hidden;
        }
        .chat-header {
            background-color: #f9fafb;
            padding: 1rem 1.5rem;
            border-bottom: 1px solid #e5e7eb;
        }
        .chat-messages {
            flex-grow: 1;
            overflow-y: auto;
            padding: 1.5rem;
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }
        .message {
            max-width: 85%;
            padding: 0.75rem 1rem;
            border-radius: 1.25rem;
            word-wrap: break-word;
        }
        .user-message {
            background-color: #3b82f6;
            color: #ffffff;
            align-self: flex-end;
            border-bottom-right-radius: 0.25rem;
        }
        .ai-message {
            background-color: #e5e7eb;
            color: #374151;
            align-self: flex-start;
            border-bottom-left-radius: 0.25rem;
        }
        .input-area {
            padding: 1.5rem;
            border-top: 1px solid #e5e7eb;
            background-color: #f9fafb;
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }
        .speech-button {
            padding: 1rem;
            background-color: #4f46e5;
            color: white;
            border: none;
            border-radius: 9999px;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.2s ease-in-out;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
        }
        .speech-button:hover {
            background-color: #4338ca;
            transform: translateY(-2px);
        }
        .speech-button:disabled {
            background-color: #9ca3af;
            cursor: not-allowed;
            transform: none;
            box-shadow: none;
        }
        .loading-dots {
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 0.5rem;
        }
        .loading-dot {
            width: 0.75rem;
            height: 0.75rem;
            background-color: #9ca3af;
            border-radius: 50%;
            animation: bounce 1s infinite;
        }
        .loading-dot:nth-child(2) { animation-delay: 0.2s; }
        .loading-dot:nth-child(3) { animation-delay: 0.4s; }
        @keyframes bounce {
            0%, 100% { transform: translateY(0); }
            50% { transform: translateY(-10px); }
        }
    </style>
</head>
<body>
    <div class="chat-container">
        <!-- Header -->
        <div class="chat-header rounded-t-2xl">
            <h1 class="text-xl font-bold text-gray-700">AI Voice Chat</h1>
            <p class="text-sm text-gray-500 mt-1">Talk to me! I'll listen and respond.</p>
        </div>

        <!-- Chat Messages -->
        <div id="chat-messages" class="chat-messages">
            <div class="ai-message">Hello! I'm ready to chat. Click the microphone button to start talking.</div>
        </div>

        <!-- Input Area -->
        <div class="input-area rounded-b-2xl">
            <div id="status-message" class="text-center text-sm font-medium text-gray-500">
                Ready to chat.
            </div>
            <button id="speech-button" class="speech-button">
                Start Talking
            </button>
        </div>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            // NOTE: The API key is provided by the runtime environment.
            // If you are running this code locally, you would replace the empty string with your own API key.
            const apiKey = "";AIzaSyBgrKqJixLyym04uZOu5FWX50mtCGDqugA
            const apiBaseUrl = 'https://generativelanguage.googleapis.com/v1beta/models';

            // DOM elements
            const chatMessagesEl = document.getElementById('chat-messages');
            const speechButton = document.getElementById('speech-button');
            const statusMessageEl = document.getElementById('status-message');

            // Set up Speech Recognition
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            let recognition = null;
            if (SpeechRecognition) {
                recognition = new SpeechRecognition();
                recognition.continuous = false; // Capture a single phrase
                recognition.interimResults = false;
                recognition.lang = 'en-US';
            } else {
                statusMessageEl.textContent = 'Speech Recognition is not supported by your browser.';
                speechButton.disabled = true;
                return;
            }

            let isListening = false;
            let audioPlayer = new Audio();
            let isPlaying = false;

            // --- UI Functions ---
            const appendMessage = (text, sender) => {
                const messageEl = document.createElement('div');
                messageEl.textContent = text;
                messageEl.classList.add('message', sender === 'user' ? 'user-message' : 'ai-message');
                chatMessagesEl.appendChild(messageEl);
                chatMessagesEl.scrollTop = chatMessagesEl.scrollHeight; // Auto-scroll to bottom
            };

            const setStatus = (message) => {
                statusMessageEl.innerHTML = message;
            };

            const toggleButtonState = (listening, disabled) => {
                isListening = listening;
                speechButton.textContent = isListening ? 'Stop Talking' : 'Start Talking';
                speechButton.disabled = disabled;
                speechButton.classList.toggle('bg-red-500', isListening);
                speechButton.classList.toggle('bg-violet-600', !isListening);
            };

            const showLoadingDots = () => {
                setStatus(`
                    Thinking
                    <span class="loading-dots">
                        <span class="loading-dot bg-gray-400"></span>
                        <span class="loading-dot bg-gray-400"></span>
                        <span class="loading-dot bg-gray-400"></span>
                    </span>
                `);
            };

            // --- API Calls ---

            // Function to handle exponential backoff for API calls
            const callApiWithBackoff = async (func, maxRetries = 5, delay = 1000) => {
                for (let i = 0; i < maxRetries; i++) {
                    try {
                        const result = await func();
                        return result;
                    } catch (error) {
                        if (error.status === 429 && i < maxRetries - 1) {
                            await new Promise(res => setTimeout(res, delay));
                            delay *= 2;
                        } else {
                            throw error;
                        }
                    }
                }
            };

            // Main function to get AI response (text and audio)
            const getAiResponse = async (text) => {
                showLoadingDots();
                
                // 1. Get Text Response from LLM
                const textPayload = {
                    contents: [{ parts: [{ text: text }] }],
                    tools: [{ "google_search": {} }],
                    systemInstruction: {
                        parts: [{ text: "You are a friendly and conversational AI companion. Keep your responses concise and helpful. Respond in a way that is good for a voice chat. Do not include markdown." }]
                    },
                };
                
                const textUrl = `${apiBaseUrl}/gemini-2.5-flash-preview-05-20:generateContent?key=${apiKey}`;

                const textResponse = await callApiWithBackoff(() => fetch(textUrl, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(textPayload)
                }));

                const textResult = await textResponse.json();
                const aiText = textResult?.candidates?.[0]?.content?.parts?.[0]?.text;

                if (!aiText) {
                    setStatus('Could not get a response from the AI. Please try again.');
                    toggleButtonState(false, false);
                    return;
                }

                appendMessage(aiText, 'ai');

                // 2. Get Audio Response from TTS
                const audioPayload = {
                    contents: [{ parts: [{ text: aiText }] }],
                    generationConfig: {
                        responseModalities: ["AUDIO"],
                        speechConfig: {
                            voiceConfig: { prebuiltVoiceConfig: { voiceName: "Kore" } }
                        }
                    },
                    model: "gemini-2.5-flash-preview-tts"
                };

                const audioUrl = `${apiBaseUrl}/gemini-2.5-flash-preview-tts:generateContent?key=${apiKey}`;

                const audioResponse = await callApiWithBackoff(() => fetch(audioUrl, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(audioPayload)
                }));

                const audioResult = await audioResponse.json();
                const audioData = audioResult?.candidates?.[0]?.content?.parts?.[0]?.inlineData?.data;

                if (audioData) {
                    // Convert base64 PCM data to a playable WAV blob
                    const audioBlob = pcmToWav(audioData, 16000, 1);
                    const audioUrl = URL.createObjectURL(audioBlob);
                    
                    audioPlayer.src = audioUrl;
                    audioPlayer.play().catch(e => console.error("Audio playback error:", e));

                    audioPlayer.onended = () => {
                        isPlaying = false;
                        setStatus('Ready to chat.');
                        toggleButtonState(false, false);
                    };
                } else {
                    setStatus('AI response is ready, but audio could not be generated. Please try again.');
                    toggleButtonState(false, false);
                }
            };
            
            // Utility function to convert Base64 to ArrayBuffer
            const base64ToArrayBuffer = (base64) => {
                const binaryString = window.atob(base64);
                const len = binaryString.length;
                const bytes = new Uint8Array(len);
                for (let i = 0; i < len; i++) {
                    bytes[i] = binaryString.charCodeAt(i);
                }
                return bytes.buffer;
            };

            // Utility function to convert raw PCM to WAV format
            const pcmToWav = (base64Audio, sampleRate, numChannels) => {
                const pcmData = base64ToArrayBuffer(base64Audio);
                const view = new DataView(new ArrayBuffer(44 + pcmData.byteLength));
                let offset = 0;

                const writeString = (str) => {
                    for (let i = 0; i < str.length; i++) {
                        view.setUint8(offset++, str.charCodeAt(i));
                    }
                };

                const writeUint32 = (val) => {
                    view.setUint32(offset, val, true);
                    offset += 4;
                };

                const writeUint16 = (val) => {
                    view.setUint16(offset, val, true);
                    offset += 2;
                };

                // RIFF chunk descriptor
                writeString('RIFF');
                writeUint32(36 + pcmData.byteLength);
                writeString('WAVE');

                // fmt chunk
                writeString('fmt ');
                writeUint32(16);
                writeUint16(1); // Audio format 1=PCM
                writeUint16(numChannels);
                writeUint32(sampleRate);
                writeUint32(sampleRate * numChannels * 2); // byte rate
                writeUint16(numChannels * 2); // block align
                writeUint16(16); // bits per sample

                // data chunk
                writeString('data');
                writeUint32(pcmData.byteLength);

                const pcmBytes = new Uint8Array(pcmData);
                for (let i = 0; i < pcmBytes.length; i++) {
                    view.setUint8(offset++, pcmBytes[i]);
                }

                return new Blob([view], { type: 'audio/wav' });
            };

            // --- Event Listeners ---
            speechButton.addEventListener('click', () => {
                if (isListening) {
                    recognition.stop();
                    toggleButtonState(false, true); // Disable button while processing
                    setStatus('Processing...');
                } else {
                    // If audio is playing, stop it first
                    if (isPlaying) {
                        audioPlayer.pause();
                        audioPlayer.currentTime = 0;
                        isPlaying = false;
                    }
                    recognition.start();
                    toggleButtonState(true, false);
                    setStatus('Listening...');
                }
            });

            // Speech recognition events
            recognition.onstart = () => {
                setStatus('Listening...');
                toggleButtonState(true, false);
            };

            recognition.onresult = (event) => {
                const transcript = event.results[0][0].transcript;
                if (transcript) {
                    appendMessage(transcript, 'user');
                    getAiResponse(transcript);
                } else {
                    setStatus('Could not understand your voice. Please try again.');
                    toggleButtonState(false, false);
                }
            };

            recognition.onerror = (event) => {
                console.error("Speech Recognition Error:", event.error);
                if (event.error === 'not-allowed') {
                    setStatus('Permission to use the microphone was denied. Please allow microphone access in your browser settings.');
                } else if (event.error === 'no-speech') {
                    setStatus('No speech was detected. Please try again and speak clearly.');
                } else {
                    setStatus(`Error: ${event.error}. Please try again.`);
                }
                toggleButtonState(false, false);
            };

            // When recognition ends
            recognition.onend = () => {
                // The button state is already handled by onresult or onerror.
                // This is a failsafe.
                if (statusMessageEl.textContent === 'Listening...') {
                    setStatus('Stopped listening.');
                    toggleButtonState(false, false);
                }
            };
        });
    </script>
</body>
</html>
